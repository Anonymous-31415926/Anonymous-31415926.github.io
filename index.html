<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

        
        <!-- Page Title -->
        <title>Rerender A Video</title>

        <!-- Favicons -->
        <meta name="msapplication-TileColor" content="#da532c">
        <meta name="theme-color" content="#ffffff">

        <!-- Vendor Stylesheets -->
        <link href="https://fonts.googleapis.com/css?family=Oswald:300,400,500,700%7CRoboto:300,400,700" rel="stylesheet">
        <link href="./assets/css/video_compare.css" rel="stylesheet">
        <!-- Theme Stylesheets -->
        <link href="./assets/css/theme.css" rel="stylesheet">

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'UA-22940424-1');
        </script>
    </head>

    <body>
        <!-- Preloader -->
        <div class="preloader">
            <div class="spinner">
                <div class="circles"></div>
            </div>
        </div>
        <!-- End of Preloader -->

        <!-- Main Content -->
        <main class="main minh-100vh">
            <!-- Section -->
            <!--31, 39, 73, .5-->
            <section class="py-0 overflow-hidden text-center">
            	<style>.bg-container.overlay::before {background-color: rgba(42, 50, 58, 1.0);}</style>
                <div class="bg-container overlay parallax" data-rellax-percentage="0.5">
                    <div class="bg-video" id="bgvideocontainer">
                    	<div class="vimeo_player_wrapper" style="position: absolute; z-index: 0; width: 100%; height: 100%; left: 0px; top: 0px; overflow: hidden; opacity: 0.5;" data-vimeo-initialized="true">
                    		<video id="bgvideo" muted autoplay="autoplay" loop="loop" src="./img/overview.mp4" style="width: auto; height: 100%;"></video>
                    	</div>
                    </div>
                </div>
                <div class="container">
                    <div class="row h-100vh align-items-center">                  	
                        <div class="col">
                            <h1 class="text-white text-uppercase fs-4 fs-lg-6" style="text-shadow: 0 4px 9px #000, 0 4px 9px #000, 0px -2px 1px #000;">Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation</h1>
                            <p class="mb-5 fs-lg-1 text-white" style="text-shadow: 0 4px 9px #000, 0px -2px 1px #000;"> <br/></p>                       
                        </div>
                    </div>
                </div>
            </section>
            <!-- End of Section -->

            <!-- Section Abstract -->
			<section class="pt-7 pb-7">
                <div class="container">
                    <div class="row justify-content-between">
                        <div class="col-lg-12">
                            <h3 class="my-0 fs-1 fw-medium text-primary text-uppercase text-center text-lg-left">Paper</h3>
                            <h2 class="mb-5 fw-medium text-secondary text-uppercase text-center text-lg-left">Abstract</h2>
                            <p class="text-justify">
                            	Large text-to-image diffusion models have exhibited impressive proficiency in generating high-quality images. However, when applying these models to video domain, ensuring temporal consistency across video frames remains a formidable challenge. This paper proposes a novel zero-shot text-guided video-to-video translation framework to adapt image models to videos. The framework includes two parts: key frame translation and full video translation. The first part uses an adapted diffusion model to generate key frames, with hierarchical cross-frame constraints applied to enforce coherence in shapes, textures and colors. The second part propagates the key frames to other frames with temporal-aware patch matching and frame blending. Our framework achieves global style and local texture temporal consistency at a low cost (without re-training or optimization). The adaptation is compatible with existing image diffusion techniques, allowing our framework to take advantage of them, such as customizing a specific subject with LoRA, and introducing extra spatial guidance with ControlNet. Extensive experimental results demonstrate the effectiveness of our proposed framework over existing methods in rendering high-quality and temporally-coherent videos.
                            </p>
			    <p class="text-justify">
                            	* Web demo: <a href="https://huggingface.co/spaces/Anonymous-sub/Rerender"><img src="https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-md-dark.svg" alt="Web Demo"></a> <br/>
                            	* Full code: According to the Anonymity Policy, we will release the full code and data upon the publication of the paper.
                            </p> 
                        </div>
 						<div class="col-lg-12 pt-3">
 							<video muted autoplay="autoplay" loop="loop" src="./img/teasers2.mp4" style="width: 100%; height: auto;"></video> 
                        </div>                       
                    </div>
                </div>
            </section>
            <!-- End of Section --> 

            <!-- Section Methodology -->
            <section class="pt-6 pb-0">
                <div class="container">
                    <div class="row">
                        <div class="col-lg-12 pr-3">
                            <div class="pb-6 pt-6 py-lg-3">
                                <h3 class="my-0 fs-1 fw-medium text-primary text-uppercase text-center text-lg-left">The</h3>
                                <h2 class="mb-5 fw-medium text-secondary text-uppercase text-center text-lg-left">Hierarchical Cross-Frame Constraints</h2>
                                <p class="text-justify">
                                     We propose novel hierarchical cross-frame constraints for pre-trained image diffusion models to produce coherent video frames. Our key idea is to use optical flow to apply dense cross-frame constraints, with the previous rendered frame serving as a low-level reference for the current frame and the first rendered frame acting as an anchor to regulate the rendering process to prevent deviations from the initial appearance. Hierarchical cross-frame constraints are realized at different stages of diffusion sampling. In addition to global style consistency (cross-frame attention), our method enforces consistency in shapes (shape-aware  cross-frame latent fusion), textures (pixel-aware  cross-frame latent fusion) and colors (color-aware adaptive latent adjustment) at early, middle and late stages, respectively. This innovative and lightweight modification achieves both global and local temporal consistency. 
                                </p>
                            </div>
                        </div>
                        <div class="col-lg-12"> 
                            <div class="mb-7">
                                <img src="./img/framework.jpg" class="mt-4 img-fluid w-100" alt="" />
                            </div>                                                         
                        </div>          
                    </div>
                </div>
            </section>
            <!-- End of Section -->  

            <!-- Section Results -->
            <section class="pt-6 pb-6"  style="background:rgb(42, 50, 58) !important">
                <div class="container">
                    <div class="row">
                        <div class="col-lg-12 order-lg-1">
                            <div class="pb-1 pt-6 py-lg-3">
                                <h3 class="my-0 fs-1 fw-medium text-primary text-uppercase text-center text-lg-left">Effect of</h3>
                                <h2 class="mb-5 fw-medium text-white text-uppercase text-center text-lg-left">Temporal Consistency Constraints</h2>
                                <p class="text-justify text-white">
                                     Our framework explores several temporal consistency constraints to generate smooth results. Their role is studied below. Move the mouse (computer) or double-click (phone) over the video to make a better comparison.
                                </p>
                            </div>
                        </div>              
                    </div>
                    <div class="row">
                        <div class="col-lg-8 order-lg-2 py-lg-3 px-0">
                            <div id="header_content-container" class="header_content-container">
	                            <div id="video-compare-container" class="header_video-comparison">
		                        	<video muted autoplay="autoplay" loop="loop" src="./img/woman_full.mp4" style="width: 100%; height: auto;" id="video-content"></video>           
			                        <div id="divider" class="divider">
			                        	<div id="divider-line" class="divider-line">
			                        		<div class="runner-animation-trigger-mobile"></div>
			                        		<div class="runner-mobile"></div>
			                        		<div class="runner"></div>
			                        	</div>
			                        </div>
									<div id="video-clipper">
									 	<video muted autoplay="autoplay" loop="loop" src="./img/woman_bl.mp4" style="width: 100%; height: auto;" id="video-content2"></video>
									</div>
								</div>
								<p class="text-justify text-white" id="video-text">
                    			Left: Original image model (Stable Diffusion). Right: Our adapted video model.
                				</p>
							</div>
                        </div>
                        <div class="col-lg-4 order-lg-1">
                            <div class="pb-6 py-lg-3" data-toggle="sticky" style="">
                                <h5 class="mb-4 text-uppercase text-primary fw-medium">Temporal Consistency Constraints</h5>
                                <ul class="mb-6 list-unstyled text-white filter">
                                    <li class="mb-1"><a href="javascript:changeVid(0);" class="text-white" id="button-show-all0">Baseline vs. Full model</a></li>
                                    <li class="mb-1"><a href="javascript:changeVid(1);" class="text-white" id="button-show-all1">Cross-Frame Attention</a></li>
                                    <li class="mb-1"><a href="javascript:changeVid(2);"class="text-white" id="button-show-all2">Color-Aware Adaptive Latent Adjustment</a></li>
                                    <li class="mb-1"><a href="javascript:changeVid(3);" class="text-white" id="button-show-all3">Shape-Aware Cross-Frame Latent Fusion</a></li>
                                    <li class="mb-1"><a href="javascript:changeVid(4);" class="text-white" id="button-show-all4">Pixel-Aware Cross-Frame Latent Fusion</a></li>
                                    <li class="mb-1"><a href="javascript:changeVid(5);" class="text-white" id="button-show-all5">Frame Propogation</a></li>
                                </ul>
                            </div>
                        </div>
                    </div>               
                </div>
            </section>
            <!-- End of Section -->

            <!-- Section Results -->
            <section class="pt-6 pb-5" style="background:#F5F5F5 !important">
                <div class="container">
                    <div class="row">
                        <div class="col-lg-12 order-lg-1">
                            <h3 class="my-0 fs-1 fw-medium text-primary text-uppercase text-center text-lg-left">Experimental</h3>
                            <h2 class="mb-5 fw-medium text-secondary text-uppercase text-center text-lg-left">Results</h2>
                            <h4 class="text-justify">
                                Comparison with zero-shot text-guided video translation methods
                            </h4><br/>
                        </div>                        
                    </div>
	                <div class="row">                                
	                    <div class="col-sm small-4 columns pb-3">
	                    	<div class="d-block border-0 aos-init aos-animate" data-aos="zoom-in">
	                        <video muted autoplay="autoplay" loop="loop" src="./img/comparison_1.mp4" style="width: 100%; height: auto;"></video> 
	                        <video muted autoplay="autoplay" loop="loop" src="./img/comparison_2.mp4" style="width: 100%; height: auto;"></video> 	   
	                        <video muted autoplay="autoplay" loop="loop" src="./img/comparison_3.mp4" style="width: 100%; height: auto;"></video> 		
	                        <video muted autoplay="autoplay" loop="loop" src="./img/comparison_4.mp4" style="width: 100%; height: auto;"></video> 	                                        
	                        <div class="card-body p-0 pt-2">
                                <p class="card-text fs--1 text-justify">
									We compare with four recent zero-shot methods: <a href="https://github.com/baaivision/vid2vid-zero" class="h6 fs--1 text-primary">vid2vid-zero</a>, <a href="https://github.com/ChenyangQiQi/FateZero" class="h6 fs--1 text-primary">FateZero</a>, <a href="https://github.com/G-U-N/Pix2Video.pytorch" class="h6 fs--1 text-primary">Pix2Video</a>, <a href="https://github.com/Picsart-AI-Research/Text2Video-Zero" class="h6 fs--1 text-primary">Text2Video-Zero</a>. <a href="https://github.com/ChenyangQiQi/FateZero" class="h6 fs--1 text-primary">FateZero</a> successfully reconstructs the input frame but fails to adjust it to match the prompt. On the other hand, <a href="https://github.com/baaivision/vid2vid-zero" class="h6 fs--1 text-primary">vid2vid-zero</a> and <a href="https://github.com/G-U-N/Pix2Video.pytorch" class="h6 fs--1 text-primary">Pix2Video</a> excessively modify the input frame, leading to significant shape distortion and discontinuity across frames. While each frame generated by <a href="https://github.com/ChenyangQiQi/FateZero" class="h6 fs--1 text-primary">FateZero</a> exhibits high quality, they lack coherence in local textures. Finally, our proposed method demonstrates clear superiority in terms of output quality, content and prompt matching and temporal consistency.
                                </p>
                            </div>
	                     </div>
	                    </div>
	                </div>      
                    <div class="row mt-4">
                        <div class="col-lg-12 order-lg-1">
                            <h4 class="text-justify">
                                More results
                            </h4><br/>
                        </div>                        
                    </div>
	                <div class="row">                                
                    <div class="row justify-content-center">
                        <div class="col-lg-12">
                            <div class="row">
                                <div class="col-lg-12 mb-4">
                                    <video muted autoplay="autoplay" loop="loop" src="./img/more_result_1.mp4" style="width: 100%; height: auto;"></video>
                                    <video muted autoplay="autoplay" loop="loop" src="./img/more_result_2.mp4" style="width: 100%; height: auto;"></video>
                                    <video muted autoplay="autoplay" loop="loop" src="./img/more_result_3.mp4" style="width: 100%; height: auto;"></video>
                                </div>
                            </div>                           
                        </div>
                    </div> 
	                </div>  	                             
                </div>
            </section>
            <!-- End of Section --> 


        </main>
        <!-- End of Main Content -->



        <!-- Core Javascripts -->
        <script src="./assets/vendor/jquery/dist/jquery.min.js"></script>

        <!-- Theme Javascripts -->
        <script src="./assets/js/theme.js"></script>

        <!-- Image Slider Javascripts -->
        <script>
			window.onload=function(){
				document.getElementById('button-show-all0').click();
				vidContainer = document.getElementById('bgvideocontainer');
			    vidContent = document.getElementById('bgvideo');
			    var newHeight = (vidContainer.offsetWidth/vidContent.videoWidth) * vidContent.videoHeight;
				vidContent.style.height = parseInt(Math.max(newHeight, vidContainer.offsetHeight)) + "px";

			    document.getElementById("video-clipper").style.width = "50%";
       			document.getElementById("divider").style.width = "50%";  
			    document.getElementById("video-clipper").getElementsByTagName("video")[0].style.width = "200%";
			    document.getElementById("video-clipper").getElementsByTagName("video")[0].style.zIndex = 3;	
			}	
		</script>		
        <script>
            $(window).on('load',function() {
              $("#images").twentytwenty();
            });
        </script>
        <script>
            $(function(){
              $(".twentytwenty-container[data-orientation!='vertical']").twentytwenty({default_offset_pct: 0.49, before_label: 'Low-Resolution', after_label: 'Super-Resolution'});
            });
        </script>
        <script>
        	window.addEventListener("resize", function() {
			    vidContainer = document.getElementById('bgvideocontainer');
			    vidContent = document.getElementById('bgvideo');
			    var newHeight = (vidContainer.offsetWidth/vidContent.videoWidth) * vidContent.videoHeight;
				vidContent.style.height = parseInt(Math.max(newHeight, vidContainer.offsetHeight)) + "px";
			});
		</script>
		<script>
			function trackLocation(e) {
			var rect = videoContainer.getBoundingClientRect(),
			      position = ((e.pageX - rect.left) / videoContainer.offsetWidth)*100;
			var offset = $("#header_content-container").offset();
    		var height = e.pageY - offset.top;			      
			  if (position <= 100) { 
			    videoClipper.style.width = position+"%";
       			divider.style.width = position + "%";
        		runner.style.marginTop = height + "px";	   
			    clippedVideo.style.width = ((100/position)*100)+"%";
			    clippedVideo.style.zIndex = 3;
				}
			}
			var videoContainer = document.getElementById("video-compare-container"),
			videoClipper = document.getElementById("video-clipper"),
			clippedVideo = videoClipper.getElementsByTagName("video")[0];
			runner = document.getElementsByClassName("runner")[0],
            divider = document.getElementById("divider");
			videoContainer.addEventListener( "mousemove", trackLocation, false); 
			videoContainer.addEventListener("touchstart",trackLocation,false);
			videoContainer.addEventListener("touchmove",trackLocation,false);


			var v = new Array();
			v[0] = [
			        "img/woman_bl.mp4",
			        "img/woman_full.mp4",
			        "Left: original Stable Diffusion (baseline). Right: our full model."
			        ];
			v[1] = [
			        "img/woman_bl.mp4",
			        "img/woman_bl_cfattn.mp4",
			        "Left: original Stable Diffusion (baseline). Right: baseline + cross-frame attention. The cross-frame attention ensures consistency in global style."
			        ];
			v[2] = [
			        "img/woman_bl_cfattn.mp4",
			        "img/woman_bl_cfattn_adain.mp4",
			        "Left: baseline + crss-frame attention. Right: left + adaptive latent adjustment. Adaptive latent adjustment maintains the same hair color as the first frame, or the hair color will turn dark." 
			        ];
			v[3] = [
			        "img/woman_bl_cfattn_adain.mp4",
			        "img/woman_bl_cfattn_adain_safusion.mp4",
			        "Left: baseline + crss-frame attention + adaptive latent adjustment. Right: left + shape-aware latent fusion. Shape-aware latent fusion can capture local rough movement, such as translating the neck ring"
			        ];

			v[4] = [
			        "img/woman_bl_cfattn_adain_safusion.mp4",
			        "img/woman_bl_cfattn_adain_safusion_pafusion.mp4",
			        "Left: adapated video model without pixel-aware latent fusion. Right: our adapted video model.  Pixel-aware latent fusion can coherently render local details such as hair styles and acne."
			        ];
			v[5] = [
			        "img/woman_bl_cfattn_adain_safusion_pafusion.mp4",
			        "img/woman_full.mp4",
			        "Left: our adapted video model. Right: our adpted video model for key frame translation and frame propagation for non-key frame translation (our full model). Hybrid diffusion-based generation and patch-based propagation can generate more smooth results and accelerate the translation process."
			        ];

			function changeVid(n){
			    var video = document.getElementById('video-content');
			    video.setAttribute("src", v[n][1]);
			    var video2 = document.getElementById('video-content2');
			    video2.setAttribute("src", v[n][0]);
			    video.load();
			    video2.load();
			    document.getElementById('video-text').innerHTML = v[n][2];
			    document.getElementById('button-show-all'+n).style.textDecoration = "underline";
			    for (let i = 1; i <= 5; i++) {
				  document.getElementById('button-show-all'+((n+i)%6)).style.textDecoration = "none";
				}
			}
					
		</script>
    </body>
</html>
